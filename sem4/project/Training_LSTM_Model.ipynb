{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxPlu8T8DbLM"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZHMhpGLDffr",
        "outputId": "6c167085-7a53-4dda-a70a-b97d73f0606d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LNn377dDbLR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "X = pd.read_csv( '/content/drive/MyDrive/maindataset/training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "y = X['domain1_score']\n",
        "X = X.dropna(axis=1)\n",
        "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "temp = pd.read_csv('/content/drive/MyDrive/maindataset/Processed_data.csv')\n"
      ],
      "metadata": {
        "id": "85PS76znLzM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X['domain1_score']=temp['final_score']"
      ],
      "metadata": {
        "id": "AOqOHh-gMBqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "REabUwAWDbLX",
        "outputId": "288c26f3-5e6b-4834-af73-595b3aba2b82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   essay_id  essay_set                                              essay  \\\n",
              "0         1          1  Dear local newspaper, I think effects computer...   \n",
              "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
              "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
              "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
              "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
              "\n",
              "   domain1_score  \n",
              "0              6  \n",
              "1              7  \n",
              "2              5  \n",
              "3              8  \n",
              "4              6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14197865-38f8-46e6-8ec9-f8a4845140f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14197865-38f8-46e6-8ec9-f8a4845140f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-14197865-38f8-46e6-8ec9-f8a4845140f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-14197865-38f8-46e6-8ec9-f8a4845140f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dde07216-4ab5-4a1c-8b86-d087c278eb4a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dde07216-4ab5-4a1c-8b86-d087c278eb4a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dde07216-4ab5-4a1c-8b86-d087c278eb4a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X",
              "summary": "{\n  \"name\": \"X\",\n  \"rows\": 12976,\n  \"fields\": [\n    {\n      \"column\": \"essay_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6309,\n        \"min\": 1,\n        \"max\": 21633,\n        \"num_unique_values\": 12976,\n        \"samples\": [\n          9908,\n          9872,\n          305\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"essay_set\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 8,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2,\n          6,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"essay\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12972,\n        \"samples\": [\n          \"The features of the setting affect the cyclist in many ways. He is riding his bike along the road and encounters a dilema.\\u0094\\u0085 @CAPS1 weeds crossed my path and a ridiculously large snake. Blocked the\\u0085 pavement in front of me.\\u0094 He had to stop and into by the snake which showed him down and freaked him out. He had problems with thirst already and had to deal with a snake that \\u0093really did look like a diamondback.\\u0094 @CAPS2, he sees a building up ahead and discovers that it\\u0092s a walrus Grape Juice factory. He thought that is might have some water but realized it was abandoned. Relating to his thirst, before he left he grabbed some pebbles.\\u0094 I\\u0092d read once that sucking on stones helps takes your mind off thirst by allowing what spit you have left to circulate.\\u0094 * @CAPS3 he came across a fishing camp and received instructions on how to go on. This affected him because he @CAPS3 got to a secure place and got proper directions and will learn from his mistake of listening to the older men at the beginning. The features of \",\n          \"The author concludes the story with this paragraph because the cold was brufield or the  author never  wanted to be around while the cold was in sesion. At the  time it was realy cold\",\n          \"Computers, a very much talked about subject. Did you know that @PERCENT1 of homes in @LOCATION1 own at least one computer. And that goes for about @PERCENT2 in @LOCATION2. Some people don't like that this is true, but on the other hand some people do. I have a computer, it can do so much, for example it can help me with writing, I can play games on it and socialize with social networking sites and \\\"I.M.'s\\\", it can even help me order a product form a website like @ORGANIZATION1, @LOCATION3, or @LOCATION4. Like I said, I have a computer and I can use it to help me write. At school in english, I would use my computer to help me with a writing prompt. I also use it to write conclusions for science labs. There are a lot of other kids that do this as well, it really is very helpful. That is only one of the reasons I think computers are beneficial to society. Another reason I think computers are beneficial to society is because you can play games on them, like \\\"world of warcraft\\\", \\\"@CAPS1 @CAPS2 @NUM1\\\", or classics like \\\"@CAPS3 @CAPS4 @CAPS5\\\". There are millions of people that play games on the computer. Also you can socialize with social networking sites, like @CAPS6 and twitter, and also because you can \\\"I.M.\\\" your friends. I.M. stands for @CAPS7 messaging, and it is kind of like email except it is just about instantaneous, and so you and friend can have a conversation over the computer. Also there is video chatting, where it is kind of like a phone except you can see the person you're talking to and it is over the computer, so it is almost like you and the person you are videochatting with are meeting face to face, but your not. A third reason I feel that computers are beneficial to society is because you can order a product over the internet form shop websites. For example, I got my @CAPS8 from @LOCATION3, and also the bulb for a @CAPS9 in my house is wearing out, so my dad is going to buy a @LOCATION1 bulb on @ORGANIZATION1. It is things like this that make life that much easier for the @CAPS10 @PERSON1, because this way these people don't have to drive to the store to buy something, that is, if they can wait a week or two for the product to arrive. These are the kinds of things that make me feel that computers are beneficial to society. I know they might not be the best reasons, but they are quite important to me. Being able to type essays, play games and socialize, and order products form websites is just amazing. These are some of the reasons that I and others love computers. But I have a question for you, why do you like or dislike computers?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"domain1_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 10,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          10,\n          6,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJeXU8zUDbLb"
      },
      "source": [
        "## Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52coBt0DDbLc"
      },
      "source": [
        "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
        "\n",
        "These are all helper functions used to clean the essays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ5nbyNMDbLd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)\n",
        "\n",
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences\n",
        "\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LSTM_layer:\n",
        "    def __init__(self, input_size, hidden_size, dropout_prob=0.4):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.W_f = np.random.randn(input_size, hidden_size)\n",
        "        self.W_i = np.random.randn(input_size, hidden_size)\n",
        "        self.W_c = np.random.randn(input_size, hidden_size)\n",
        "        self.W_o = np.random.randn(input_size, hidden_size)\n",
        "        self.U_f = np.random.randn(hidden_size, hidden_size)\n",
        "        self.U_i = np.random.randn(hidden_size, hidden_size)\n",
        "        self.U_c = np.random.randn(hidden_size, hidden_size)\n",
        "        self.U_o = np.random.randn(hidden_size, hidden_size)\n",
        "        self.b_f = np.zeros(hidden_size)\n",
        "        self.b_i = np.zeros(hidden_size)\n",
        "        self.b_c = np.zeros(hidden_size)\n",
        "        self.b_o = np.zeros(hidden_size)\n",
        "\n",
        "    def forward(self, x, prev_hidden_state, prev_cell_state):\n",
        "        f = sigmoid(np.dot(x, self.W_f) + np.dot(prev_hidden_state, self.U_f) + self.b_f)\n",
        "        i = sigmoid(np.dot(x, self.W_i) + np.dot(prev_hidden_state, self.U_i) + self.b_i)\n",
        "        c_tilde = np.tanh(np.dot(x, self.W_c) + np.dot(prev_hidden_state, self.U_c) + self.b_c)\n",
        "        o = sigmoid(np.dot(x, self.W_o) + np.dot(prev_hidden_state, self.U_o) + self.b_o)\n",
        "\n",
        "        c = f * prev_cell_state + i * c_tilde\n",
        "        h = o * np.tanh(c)\n",
        "\n",
        "        return h, c\n",
        "\n",
        "class Dropout_layer:\n",
        "    def __init__(self, dropout_prob):\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = np.random.rand(*x.shape) < (1 - self.dropout_prob)\n",
        "        return x * mask / (1 - self.dropout_prob)\n",
        "\n",
        "class Dense_layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "        self.W = np.random.randn(input_size, output_size)\n",
        "        self.b = np.zeros(output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = np.dot(x, self.W) + self.b\n",
        "        if self.activation == 'relu':\n",
        "            return np.maximum(0, output)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return 1 / (1 + np.exp(-output))\n",
        "\n",
        "class Sequential_model:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def compile(self, loss, optimizer, metrics):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.metrics = metrics\n",
        "\n",
        "    def summary(self):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__}\")\n",
        "\n",
        "    def fit(self, X, y, batch_size, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                batch_X = X[i:i+batch_size]\n",
        "                batch_y = y[i:i+batch_size]\n",
        "                self.train_on_batch(batch_X, batch_y)\n",
        "\n",
        "    def train_on_batch(self, X_batch, y_batch):\n",
        "        prev_hidden_state = np.zeros((X_batch.shape[0], self.layers[0].hidden_size))  # Initial hidden state\n",
        "        prev_cell_state = np.zeros((X_batch.shape[0], self.layers[0].hidden_size))  # Initial cell state\n",
        "\n",
        "        # Forward pass\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, LSTM_layer):\n",
        "                X_batch, prev_hidden_state = layer.forward(X_batch, prev_hidden_state, prev_cell_state)\n",
        "            else:\n",
        "                X_batch = layer.forward(X_batch)\n",
        "\n",
        "        # Compute loss and perform backpropagation\n",
        "        loss = self.loss_function(X_batch, y_batch)\n",
        "        self.backward(loss)\n",
        "\n",
        "    def loss_function(self, y_pred, y_true):\n",
        "        return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def backward(self, loss):\n",
        "        pass  # Implement backpropagation here\n",
        "\n",
        "# Helper function for sigmoid activation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Example usage:\n",
        "# Define your model\n",
        "model = Sequential_model()\n",
        "model.add(LSTM_layer(input_size=300, hidden_size=300))\n",
        "model.add(Dropout_layer(dropout_prob=0.4))\n",
        "model.add(LSTM_layer(input_size=300, hidden_size=64))\n",
        "model.add(Dropout_layer(dropout_prob=0.5))\n",
        "model.add(Dense_layer(input_size=64, output_size=1, activation='relu'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGPgU7ARKBTM",
        "outputId": "59b9226f-91aa-4dbe-a13c-71d6b8f71bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: LSTM_layer\n",
            "Layer 2: Dropout_layer\n",
            "Layer 3: LSTM_layer\n",
            "Layer 4: Dropout_layer\n",
            "Layer 5: Dense_layer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Sequential_model:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss_history = []\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def compile(self, loss, optimizer, metrics):\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.metrics = metrics\n",
        "\n",
        "    def summary(self):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__}\")\n",
        "\n",
        "    def fit(self, X, y, batch_size, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, len(X), batch_size):\n",
        "                batch_X = X[i:i+batch_size]\n",
        "                batch_y = y[i:i+batch_size]\n",
        "                loss = self.train_on_batch(batch_X, batch_y)\n",
        "                epoch_loss += loss\n",
        "            epoch_loss /= len(X) / batch_size  # Average loss per sample\n",
        "            print(f\"Epoch Loss: {epoch_loss}\")\n",
        "            self.loss_history.append(epoch_loss)\n",
        "\n",
        "    def train_on_batch(self, X_batch, y_batch):\n",
        "    # Forward pass\n",
        "       y_pred = self.predict(X_batch)\n",
        "\n",
        "    # Compute loss\n",
        "       loss = np.mean((y_pred - y_batch) ** 2)\n",
        "\n",
        "    # Backpropagation\n",
        "       delta = 2 * (y_pred - y_batch) / len(y_batch)  # Derivative of mean squared error loss\n",
        "\n",
        "       for layer in reversed(self.layers):\n",
        "        delta = layer.backward(delta)  # Backward pass to compute gradients and update delta\n",
        "\n",
        "       return loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        prev_hidden_state = np.zeros((X.shape[0], self.layers[0].hidden_size))  # Initial hidden state\n",
        "        prev_cell_state = np.zeros((X.shape[0], self.layers[0].hidden_size))  # Initial cell state\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, LSTM_layer):\n",
        "                X, prev_hidden_state, prev_cell_state = layer.forward(X, prev_hidden_state, prev_cell_state)\n",
        "            else:\n",
        "                X = layer.forward(X)\n",
        "        return X\n",
        "\n",
        "    def train_on_batch(self, X_batch, y_batch):\n",
        "        prev_hidden_state = np.zeros((X_batch.shape[0], self.layers[0].hidden_size))  # Initial hidden state\n",
        "        prev_cell_state = np.zeros((X_batch.shape[0], self.layers[0].hidden_size))  # Initial cell state\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, LSTM_layer):\n",
        "                X_batch, prev_hidden_state, prev_cell_state = layer.forward(X_batch, prev_hidden_state, prev_cell_state)\n",
        "            else:\n",
        "                X_batch = layer.forward(X_batch)\n",
        "        # Update weights and compute loss\n",
        "        loss = self._backpropagation(X_batch, y_batch)\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Generate some example data\n",
        "X_train = np.random.randn(1000, 300)\n",
        "y_train = np.random.rand(1000)\n",
        "\n",
        "# Define your model\n",
        "model = Sequential_model()\n",
        "model.add(LSTM_layer(input_size=300, hidden_size=300))\n",
        "model.add(Dropout_layer(dropout_prob=0.4))\n",
        "model.add(LSTM_layer(input_size=300, hidden_size=64))\n",
        "model.add(Dropout_layer(dropout_prob=0.5))\n",
        "model.add(Dense_layer(input_size=64, output_size=1, activation='relu'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=10)\n",
        "\n",
        "# Make predictions\n",
        "X_test = np.random.randn(100, 300)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Draw loss curve\n",
        "plt.plot(model.loss_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, 1)  # Set y-axis range from 0 to 1\n",
        "plt.title('Loss Curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "97W3hOY7ICvL",
        "outputId": "852a9ff3-55d6-4945-bd29-35491bd8892a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: LSTM_layer\n",
            "Layer 2: Dropout_layer\n",
            "Layer 3: LSTM_layer\n",
            "Layer 4: Dropout_layer\n",
            "Layer 5: Dense_layer\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f2c5d8b8b5ea>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-f2c5d8b8b5ea>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m  \u001b[0;31m# Average loss per sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-f2c5d8b8b5ea>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, X_batch, y_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_cell_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_cell_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your data splitting manually\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split your data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size as needed\n",
        "\n",
        "train_essays = X_train['essay']\n",
        "test_essays = X_test['essay']\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for essay in train_essays:\n",
        "    # Obtaining all sentences from the training essays.\n",
        "    sentences += essay_to_sentences(essay, remove_stopwords=True)\n",
        "\n",
        "# Initializing variables for word2vec model.\n",
        "num_features = 300\n",
        "min_word_count = 40\n",
        "num_workers = 4\n",
        "context = 10\n",
        "downsampling = 1e-3\n",
        "\n",
        "print(\"Training Word2Vec Model...\")\n",
        "model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)\n",
        "\n",
        "model.init_sims(replace=True)\n",
        "# model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
        "\n",
        "clean_train_essays = []\n",
        "\n",
        "# Generate training and testing data word vectors.\n",
        "for essay_v in train_essays:\n",
        "    clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
        "\n",
        "clean_test_essays = []\n",
        "for essay_v in test_essays:\n",
        "    clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
        "\n",
        "trainDataVecs = np.array(trainDataVecs)\n",
        "testDataVecs = np.array(testDataVecs)\n",
        "# Reshaping train and test vectors to 3 dimensions. (1 represents one timestep)\n",
        "trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "\n",
        "lstm_model = get_model()\n",
        "lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
        "# lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
        "y_pred = lstm_model.predict(testDataVecs)\n",
        "\n",
        "# Round y_pred to the nearest integer.\n",
        "y_pred = np.around(y_pred)\n",
        "\n",
        "# Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
        "print(\"Kappa Score: {}\".format(result))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "1v_DFTBwGUeW",
        "outputId": "6fa0145c-a863-4556-c65a-2f3cdbbe78b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d3179a58312a>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0messay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_essays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Obtaining all sentences from the training essays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0messay_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initializing variables for word2vec model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2fcda7e28a96>\u001b[0m in \u001b[0;36messay_to_sentences\u001b[0;34m(essay_v, remove_stopwords)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mraw_sentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_sentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0messay_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2fcda7e28a96>\u001b[0m in \u001b[0;36messay_to_wordlist\u001b[0;34m(essay_v, remove_stopwords)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0messay_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mstops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         return [\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         ]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn6e8hDnGmhs",
        "outputId": "30a986b8-9673-487b-b191-a28d50627332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbZRfjZEGatd",
        "outputId": "ef5cd2cb-0066-4e72-e7aa-b99f0582a0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN4iVSxYDbLe"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cyTUC3LDbLf"
      },
      "source": [
        "Here we define a 2-Layer LSTM Model.\n",
        "\n",
        "Note that instead of using sigmoid activation in the output layer we will use\n",
        "Relu since we are not normalising training labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNjJfNdZDbLg"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SseX2SKtDbLh"
      },
      "source": [
        "## Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYw5un3aDbLi"
      },
      "source": [
        "Now we train the model on the dataset.\n",
        "\n",
        "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
        "We will then calculate Average Kappa for all the folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "_yDUONkjDbLi",
        "outputId": "faa45068-eaea-49c2-93d4-308dbf2199ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn.cross_validation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cc283ac4edd9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcohen_kappa_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from sklearn.cross_validation import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cv = KFold(len(X), n_folds=5, shuffle=True)\n",
        "results = []\n",
        "y_pred_list = []\n",
        "\n",
        "count = 1\n",
        "for traincv, testcv in cv:\n",
        "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
        "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
        "\n",
        "    train_essays = X_train['essay']\n",
        "    test_essays = X_test['essay']\n",
        "\n",
        "    sentences = []\n",
        "\n",
        "    for essay in train_essays:\n",
        "            # Obtaining all sentences from the training essays.\n",
        "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
        "\n",
        "    # Initializing variables for word2vec model.\n",
        "    num_features = 300\n",
        "    min_word_count = 40\n",
        "    num_workers = 4\n",
        "    context = 10\n",
        "    downsampling = 1e-3\n",
        "\n",
        "    print(\"Training Word2Vec Model...\")\n",
        "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "    model.init_sims(replace=True)\n",
        "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
        "\n",
        "    clean_train_essays = []\n",
        "\n",
        "    # Generate training and testing data word vectors.\n",
        "    for essay_v in train_essays:\n",
        "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
        "\n",
        "    clean_test_essays = []\n",
        "    for essay_v in test_essays:\n",
        "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
        "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "\n",
        "    trainDataVecs = np.array(trainDataVecs)\n",
        "    testDataVecs = np.array(testDataVecs)\n",
        "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "\n",
        "    lstm_model = get_model()\n",
        "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
        "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
        "    y_pred = lstm_model.predict(testDataVecs)\n",
        "\n",
        "    # Save any one of the 8 models.\n",
        "    # if count == 5:\n",
        "    #      lstm_model.save('./model_weights/final_lstm.h5')\n",
        "\n",
        "    # Round y_pred to the nearest integer.\n",
        "    y_pred = np.around(y_pred)\n",
        "\n",
        "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
        "    print(\"Kappa Score: {}\".format(result))\n",
        "    results.append(result)\n",
        "\n",
        "    count += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdMGaZ0BDbLk"
      },
      "source": [
        "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D9fh4rzDbLk",
        "outputId": "47fe6fbc-7c48-4cf5-83c2-8be4172b4500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Kappa score after a 5-fold cross validation:  0.9613\n"
          ]
        }
      ],
      "source": [
        "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i2QetJLDbLl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}